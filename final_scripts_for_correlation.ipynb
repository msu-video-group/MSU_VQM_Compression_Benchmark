{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Open Part Results Reproduction for Paper\n",
    "## \"Video compression dataset and benchmark of learning-based video-quality metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend import Legend\n",
    "import json\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('metric_scores_open.csv')  # read dataframe with metrics data\n",
    "with open(\"content_categories.json\") as f:\n",
    "    cat_dict = json.load(f) # read dict with \"content category\"-\"list of corresponding videos\" mapping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table with correlation coefficients for each samples group creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (!) Note that single codecs and bitrate ranges are available only for the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# choose compression standards\n",
    "codec_list = ['all'] #['all', 'h265', 'av1', 'vvc'] - for full (open + hidden parts) dataset\n",
    "\n",
    "#choose bitrate categories\n",
    "bit_cat_dict = ['all'] #['all', 'high', 'low'] - for full (open + hidden parts) dataset\n",
    "\n",
    "corrs = pd.DataFrame(columns=['corr', 'sample_size'] + list(df.columns[8:-1]))\n",
    "for comp in ['2021','2019','2020','ugc']:\n",
    "    print(comp, 'Comparison')\n",
    "\n",
    "    _df = df[df.comparison == comp]\n",
    "    for seq in tqdm.tqdm(_df.sequence.unique()):\n",
    "\n",
    "        for preset in _df.preset.unique():\n",
    "            for codec in codec_list: \n",
    "                for cat in bit_cat_dict: \n",
    "                    \n",
    "                    flt = ((df.comparison == comp) & (df.sequence == seq) & (df.preset == preset))\n",
    "                    if cat != 'all':\n",
    "                        flt = flt & (df['bitrate'] == cat)\n",
    "                    if codec != 'all':\n",
    "                        flt = flt & (df['standard'] == codec)\n",
    "                    \n",
    "                    subj = df[flt]\n",
    "                    \n",
    "                    if (subj.shape[0] < 3):\n",
    "                        continue\n",
    "                    for corr in ['spearman', 'kendall']:\n",
    "                        if (corr == 'kendall'):\n",
    "        \n",
    "                            # Pandas KROCC implementation isn't stable\n",
    "                            # in the presence of duplicates\n",
    "\n",
    "                            dct = {\n",
    "                                'comparison': str(comp),\n",
    "                                'sequence': seq,\n",
    "                                'preset': preset,\n",
    "                                'corr': corr,\n",
    "                                'standard': codec,\n",
    "                                'bitrate' : cat,\n",
    "                                'sample_size': subj.shape[0],\n",
    "                                **subj.corr(method=lambda x, y: stats.kendalltau(x, y)[0])['Subjective score']\n",
    "                            }\n",
    "                        \n",
    "                        else:\n",
    "                            \n",
    "                            dct = {\n",
    "                                'comparison': str(comp),\n",
    "                                'sequence': seq,\n",
    "                                'preset': preset,\n",
    "                                'corr': corr,\n",
    "                                'standard': codec,\n",
    "                                'bitrate' : cat,\n",
    "                                'sample_size': subj.shape[0],\n",
    "                                **subj.corr(method=corr)['Subjective score']\n",
    "                            }\n",
    "                 \n",
    "                        corrs = corrs.append(dct, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and confidence intervals computing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigh_func(col, weights, mode='mean'):\n",
    "    st = DescrStatsW(col, weights=weights)\n",
    "    l = st.mean - 1.96 * st.std_mean\n",
    "    \n",
    "    u = st.mean + 1.96 * st.std_mean\n",
    "    if st.mean > 0:\n",
    "        l = np.clip(l, 0, np.arctanh(0.99999))\n",
    "        u = np.clip(u, 0, np.arctanh(0.99999))\n",
    "    else:\n",
    "        t = u\n",
    "        u = np.clip(l, -np.arctanh(0.99999), 0)\n",
    "        l = np.clip(t, -np.arctanh(0.99999), 0)\n",
    "\n",
    "    if mode == 'mean':\n",
    "        return st.mean\n",
    "    elif mode == '-se':\n",
    "        return l\n",
    "    elif mode == '+se':\n",
    "        return u\n",
    "    else:\n",
    "        raise ValueError('Unknown mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our dataset subsets which were presented in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (!) Note that only \"FULL DATASET\" is available for the open part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = dict()\n",
    "pools[\"FULL DATASET\"] = ('all', 'all','all',cat_dict[\"FULL DATASET\"])\n",
    "pools[\"LOW BITRATE\"] = ('all', 'low','all',cat_dict[\"FULL DATASET\"])\n",
    "pools[\"HIGH BITRATE\"] = ('all', 'high','all',cat_dict[\"FULL DATASET\"])\n",
    "pools[\"H.265\"] = ('all', 'all','h265',cat_dict[\"FULL DATASET\"])\n",
    "pools[\"AV1\"] = ('all', 'all','av1',cat_dict[\"FULL DATASET\"])\n",
    "pools[\"VVC\"] = ('all', 'all','vvc',cat_dict[\"FULL DATASET\"])\n",
    "pools[\"UGC\"] = ('ugc','all','all', cat_dict[\"FULL DATASET\"])\n",
    "pools[\"SHAKING\"] = ('all','all','all', cat_dict[\"shaking\"])\n",
    "pools[\"SPORTS\"] = ('all','all','all', cat_dict[\"sports\"])\n",
    "pools[\"NATURE\"] = ('all','all','all', cat_dict[\"nature\"])\n",
    "pools[\"GAMING and ANIMATION\"] = ('all','all','all', cat_dict[\"gaming_animation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation options choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = 'spearman' #, 'kendall'\n",
    "pool = pools[\"FULL DATASET\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final results generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp, cat, codec, seq = pool\n",
    "preset = 'all'\n",
    "\n",
    "min_samples_srocc = 15\n",
    "min_samples_krocc = 6\n",
    "cols = corrs.columns[2:-6]\n",
    "\n",
    "if (corr == \"spearman\"):\n",
    "    min_samples = min_samples_srocc\n",
    "else:\n",
    "    min_samples = min_samples_krocc\n",
    "\n",
    "flt = (corrs['corr'] == corr)\n",
    "\n",
    "if type(comp) is list:\n",
    "    flt = flt & (corrs.comparison.isin(comp))\n",
    "elif comp != 'all':\n",
    "    flt = flt & (corrs.comparison == comp)\n",
    "\n",
    "if type(cat) is list:\n",
    "    flt = flt & (corrs.bitrate.isin(cat))\n",
    "else:\n",
    "    flt = flt & (corrs.bitrate == cat)\n",
    "\n",
    "if type(seq) is list:\n",
    "    flt = flt & (corrs.sequence.isin(seq))\n",
    "elif seq != 'all':\n",
    "    flt = flt & (corrs.sequence == seq)\n",
    "\n",
    "if type(codec) is list:\n",
    "    flt = flt & (corrs.standard.isin(codec))\n",
    "else:\n",
    "    flt = flt & (corrs.standard == codec)\n",
    "\n",
    "if type(preset) is list:\n",
    "    flt = flt & (corrs.preset.isin(preset))\n",
    "elif preset != 'all':\n",
    "    flt = flt & (corrs.preset == preset)\n",
    "\n",
    "flt = flt & (corrs.sample_size >= min_samples)\n",
    "\n",
    "\n",
    "d_mean_correlation = (corrs[flt][cols].apply(lambda x: np.arctanh(x)).replace([np.inf, -np.inf], [np.arctanh(0.99), np.arctanh(-0.99)])\\\n",
    ".apply(lambda x: weigh_func(x, corrs[flt]['sample_size'], 'mean')))\\\n",
    ".apply(lambda x: np.tanh(x)).abs().replace([0.99], 1).sort_values(ascending=False)\n",
    "\n",
    "d_ci_lower_bound = (corrs[flt][cols].apply(lambda x: np.arctanh(x)).replace([np.inf, -np.inf], [np.arctanh(0.99), np.arctanh(-0.99)])\\\n",
    ".apply(lambda x: weigh_func(x, corrs[flt]['sample_size'], '-se')))\\\n",
    ".apply(lambda x: np.tanh(x)).abs().replace([0.99], 1).sort_values(ascending=False)\n",
    "\n",
    "d_ci_upper_bound = (corrs[flt][cols].apply(lambda x: np.arctanh(x)).replace([np.inf, -np.inf], [np.arctanh(0.99), np.arctanh(-0.99)])\\\n",
    ".apply(lambda x: weigh_func(x, corrs[flt]['sample_size'], '+se')))\\\n",
    ".apply(lambda x: np.tanh(x)).abs().replace([0.99], 1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_final_correlation = pd.concat([d_ci_lower_bound, d_mean_correlation, d_ci_upper_bound], axis=1).reset_index()\n",
    "d_final_correlation.columns = [\"Metric\", \"CI Lower Bound\", \"Mean \"+ corr[0].upper() + corr[1:] + \" Correlation\", \"CI Upper Bound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d_final_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
